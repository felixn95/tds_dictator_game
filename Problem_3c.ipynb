{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "c)\n",
    "The questionnaire by Bruhin et al. comprises personality traits according to the Big Five. This question asks you to impute other (economic) preferences of the study participants. Examples include:\n",
    "\n",
    "Social preferences (inequity aversion, reciprocity, guilt aversion...)\n",
    "Time preferences (myopia, present bias...)\n",
    "Risk preferences\n",
    "...\n",
    "Your taks is thus the following:\n",
    "\n",
    "Find a dataset on individuals that contains the Big Five along with other preference measures. Think of datasets used in scientific publications.\n",
    "Train models to predict the other preferences from the Big Five. Evaluate their performance.\n",
    "Make an out-of sample prediction using the fitted models to impute the preference measures for the study participants of Bruhin et al."
   ],
   "id": "ca8b7edfdac5be60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First of all: We really searched through hundreds of papers in scientific databases for rudimentarily useful data sets, but always failed at the following points: \n",
    "- The papers might dealt with big-five scores, but the open data were some kind of raw data without concrete values for big-five scores or pretty advanced data sets with a lot of features and no useful big-five scores.\n",
    "- Some papers seemed really useful, but the data was not open access or not even published at all.. \n",
    "\n",
    "At least, we now found some data that might be useful for this task. But then we came to the next problem: \n",
    "\n",
    "The big-five scores in our data from Bruhin et al. cannot be clearly explained. Even in the appendix data or in the additional supplementary-data of the paper,\n",
    "we could not find any information on the questionnaire that clearly expresses what the answer options to the big-five questions are. So we do not know about the scoring system of the big-five scores.\n",
    "\n",
    "Maybe we missed something, but the scores are varying across the most papers. So we decided to just create a mapping between our found data set and the big-five scores of Bruhin et al. \n",
    "\n",
    "The data set we found: https://github.com/automoto/big-five-data\n",
    "\n",
    "The only useful variable here, that we can use for predict a information missing in our subjects, is the country of the subject. "
   ],
   "id": "992b2d4104b4c08d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T17:24:55.938495Z",
     "start_time": "2024-08-04T17:24:55.718721Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_countries = pd.read_csv('data/big_five_scores.csv')\n",
    "\n",
    "df_bruhin = pd.read_csv('data/subjects.csv')\n",
    "\n",
    "print (df_countries.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['case_id', 'country', 'age', 'sex', 'agreeable_score',\n",
      "       'extraversion_score', 'openness_score', 'conscientiousness_score',\n",
      "       'neuroticism_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T17:24:55.974681Z",
     "start_time": "2024-08-04T17:24:55.940324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's first explore the big five scores in the data set\n",
    "df_countries.head(1)"
   ],
   "id": "2697fed2711c8c34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   case_id     country  age  sex  agreeable_score  extraversion_score  \\\n",
       "0        1  South Afri   24    1         0.753333            0.496667   \n",
       "\n",
       "   openness_score  conscientiousness_score  neuroticism_score  \n",
       "0        0.803333                 0.886667           0.426667  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>agreeable_score</th>\n",
       "      <th>extraversion_score</th>\n",
       "      <th>openness_score</th>\n",
       "      <th>conscientiousness_score</th>\n",
       "      <th>neuroticism_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>South Afri</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.496667</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.426667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T17:24:56.006330Z",
     "start_time": "2024-08-04T17:24:55.974681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As also written in the README of the data set, each of their big five personality traits has a value between 0 and 1. We will apply this to the data of bruhin by normalizing the values.\n",
    "\n",
    "# List of Big Five personality traits\n",
    "big_five_traits = ['bf_consciousness', 'bf_openness', 'bf_extraversion', 'bf_agreeableness', 'bf_neuroticism']\n",
    "\n",
    "# Normalize each of the Big Five traits in df_bruhin\n",
    "for trait in big_five_traits:\n",
    "    min_val = df_bruhin[trait].min()\n",
    "    max_val = df_bruhin[trait].max()\n",
    "    df_bruhin[trait] = (df_bruhin[trait] - min_val) / (max_val - min_val)\n",
    "\n",
    "# rename the columns to match df_countries format\n",
    "df_bruhin.rename(columns={\n",
    "    'bf_consciousness': 'conscientiousness_score',\n",
    "    'bf_openness': 'openness_score',\n",
    "    'bf_extraversion': 'extraversion_score',\n",
    "    'bf_agreeableness': 'agreeable_score',\n",
    "    'bf_neuroticism': 'neuroticism_score'\n",
    "}, inplace=True)\n",
    "\n",
    "df_bruhin.head(1)"
   ],
   "id": "f94e39b430ef0b9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           sid  conscientiousness_score  openness_score  extraversion_score  \\\n",
       "0  12010050501                     0.75        0.421053              0.4375   \n",
       "\n",
       "   agreeable_score  neuroticism_score  cogabil  pe_D1_stud_natsci  \\\n",
       "0         0.866667           0.176471        3                  1   \n",
       "\n",
       "   pe_D1_stud_law  pe_D1_stud_socsci  pe_D1_stud_med  pe_monthinc  pe_age  \\\n",
       "0               0                  0               0          400      21   \n",
       "\n",
       "   pe_female  \n",
       "0          1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>conscientiousness_score</th>\n",
       "      <th>openness_score</th>\n",
       "      <th>extraversion_score</th>\n",
       "      <th>agreeable_score</th>\n",
       "      <th>neuroticism_score</th>\n",
       "      <th>cogabil</th>\n",
       "      <th>pe_D1_stud_natsci</th>\n",
       "      <th>pe_D1_stud_law</th>\n",
       "      <th>pe_D1_stud_socsci</th>\n",
       "      <th>pe_D1_stud_med</th>\n",
       "      <th>pe_monthinc</th>\n",
       "      <th>pe_age</th>\n",
       "      <th>pe_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12010050501</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:01:36.686867Z",
     "start_time": "2024-08-04T17:24:56.006330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Check the number of columns in df_countries\n",
    "num_columns = df_countries.shape[1]\n",
    "\n",
    "# Ensure the column names match\n",
    "required_columns = ['case_id', 'country', 'age', 'sex', 'agreeable_score', 'extraversion_score', \n",
    "                    'openness_score', 'conscientiousness_score', 'neuroticism_score']\n",
    "additional_columns = [f'feature_{i}' for i in range(num_columns - len(required_columns))]\n",
    "df_countries.columns = required_columns + additional_columns\n",
    "\n",
    "# Randomly select 30k columns from the dataset (ensure we include the required columns)\n",
    "if num_columns > 30000:\n",
    "    selected_columns = np.random.choice(df_countries.columns[len(required_columns):], 30000 - len(required_columns), replace=False)\n",
    "    selected_columns = required_columns + list(selected_columns)\n",
    "else:\n",
    "    selected_columns = df_countries.columns\n",
    "\n",
    "df_countries_reduced = df_countries[selected_columns]\n",
    "\n",
    "# Select relevant columns for model training\n",
    "features = ['agreeable_score', 'extraversion_score', 'openness_score', 'conscientiousness_score', 'neuroticism_score']\n",
    "X = df_countries_reduced[features]\n",
    "y = df_countries_reduced['country']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Prepare the test data from df_bruhin\n",
    "X_bruhin = df_bruhin[features]\n",
    "X_bruhin_scaled = scaler.transform(X_bruhin)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = accuracy\n",
    "    return results\n",
    "\n",
    "# Define the models to train with updated Logistic Regression parameters\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver='saga'),\n",
    "    # 'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = train_and_evaluate(models, X_train, y_train, X_test, y_test)\n",
    "print(results)\n",
    "\n",
    "# Select the best model based on the results\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X_scaled, y_encoded)\n",
    "\n",
    "# Predict the countries for the subjects in df_bruhin\n",
    "y_bruhin_pred = best_model.predict(X_bruhin_scaled)\n",
    "y_bruhin_pred_labels = label_encoder.inverse_transform(y_bruhin_pred)\n",
    "\n",
    "# Add predictions to df_bruhin\n",
    "df_bruhin['predicted_country'] = y_bruhin_pred_labels\n",
    "\n",
    "# Save the results\n",
    "df_bruhin.to_csv('data/subjects_with_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the prediction results\n",
    "print(df_bruhin.head())\n"
   ],
   "id": "f1f7bd4b991d740d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': 0.6915868083237069}\n",
      "           sid  conscientiousness_score  openness_score  extraversion_score  \\\n",
      "0  12010050501                   0.7500        0.421053              0.4375   \n",
      "1  12010050502                   0.5000        0.684211              0.4375   \n",
      "2  12010050603                   0.4375        0.473684              0.4375   \n",
      "3  12010050704                   0.6875        0.421053              0.2500   \n",
      "4  12010050705                   0.8125        0.315789              0.0000   \n",
      "\n",
      "   agreeable_score  neuroticism_score  cogabil  pe_D1_stud_natsci  \\\n",
      "0         0.866667           0.176471        3                  1   \n",
      "1         0.600000           0.647059        7                  1   \n",
      "2         0.733333           0.647059        3                  0   \n",
      "3         0.800000           0.647059        9                  1   \n",
      "4         0.933333           0.235294        4                  0   \n",
      "\n",
      "   pe_D1_stud_law  pe_D1_stud_socsci  pe_D1_stud_med  pe_monthinc  pe_age  \\\n",
      "0               0                  0               0          400      21   \n",
      "1               0                  0               0          800      21   \n",
      "2               0                  1               0          800      23   \n",
      "3               0                  0               0          500      22   \n",
      "4               0                  0               1          350      19   \n",
      "\n",
      "   pe_female predicted_country  \n",
      "0          1               USA  \n",
      "1          0               USA  \n",
      "2          0               USA  \n",
      "3          0               USA  \n",
      "4          1               USA  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next data set we found, included information about the preference of individuals regarding their longitudinal study attrition. The data set is from a scientific publication and can be found here: https://data.mendeley.com/datasets/g3jx8zt2t9/1",
   "id": "7c8ad385d4ed0aaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:01:36.814711Z",
     "start_time": "2024-08-04T18:01:36.686867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the datasets\n",
    "df_attrition = pd.read_csv('data/personality_survey_participation_MI.csv')\n",
    "df_bruhin = pd.read_csv('data/subjects.csv')\n",
    "\n",
    "# List of Big Five personality traits\n",
    "big_five_traits_attrition = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "big_five_traits_bruhin = ['bf_openness', 'bf_consciousness', 'bf_extraversion', 'bf_agreeableness', 'bf_neuroticism']\n",
    "\n",
    "# Normalize each of the Big Five traits in df_attrition\n",
    "scaler = MinMaxScaler()\n",
    "df_attrition[big_five_traits_attrition] = scaler.fit_transform(df_attrition[big_five_traits_attrition])\n",
    "\n",
    "# Normalize each of the Big Five traits in df_bruhin\n",
    "df_bruhin[big_five_traits_bruhin] = scaler.fit_transform(df_bruhin[big_five_traits_bruhin])\n",
    "\n",
    "# Rename columns in df_bruhin to match df_attrition\n",
    "df_bruhin.rename(columns={\n",
    "    'bf_openness': 'Openness',\n",
    "    'bf_consciousness': 'Conscientiousness',\n",
    "    'bf_extraversion': 'Extraversion',\n",
    "    'bf_agreeableness': 'Agreeableness',\n",
    "    'bf_neuroticism': 'Neuroticism'\n",
    "}, inplace=True)\n",
    "\n",
    "# Save the normalized df_bruhin\n",
    "df_bruhin.to_csv('data/subjects_normalized.csv', index=False)\n",
    "\n",
    "# Select relevant columns for model training\n",
    "features = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "X = df_attrition[features]\n",
    "y = df_attrition['Attrition']\n",
    "\n",
    "# Prepare the test data from df_bruhin\n",
    "X_bruhin = df_bruhin[features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = accuracy\n",
    "    return results\n",
    "\n",
    "# Define the models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver='saga'),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = train_and_evaluate(models, X_train, y_train, X_test, y_test)\n",
    "print(results)\n",
    "\n",
    "# Select the best model based on the results\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the Attrition for the subjects in df_bruhin\n",
    "y_bruhin_pred = best_model.predict(X_bruhin)\n",
    "\n",
    "# Add predictions to df_bruhin\n",
    "df_bruhin['predicted_attrition'] = y_bruhin_pred\n",
    "\n",
    "# Save the results\n",
    "df_bruhin.to_csv('data/subjects_with_attrition_predictions.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the prediction results\n",
    "print(df_bruhin.head())\n"
   ],
   "id": "575c4cd64443c041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': 0.8835758835758836, 'Decision Tree': 0.7775467775467776}\n",
      "           sid  Conscientiousness  Openness  Extraversion  Agreeableness  \\\n",
      "0  12010050501             0.7500  0.421053        0.4375       0.866667   \n",
      "1  12010050502             0.5000  0.684211        0.4375       0.600000   \n",
      "2  12010050603             0.4375  0.473684        0.4375       0.733333   \n",
      "3  12010050704             0.6875  0.421053        0.2500       0.800000   \n",
      "4  12010050705             0.8125  0.315789        0.0000       0.933333   \n",
      "\n",
      "   Neuroticism  cogabil  pe_D1_stud_natsci  pe_D1_stud_law  pe_D1_stud_socsci  \\\n",
      "0     0.176471        3                  1               0                  0   \n",
      "1     0.647059        7                  1               0                  0   \n",
      "2     0.647059        3                  0               0                  1   \n",
      "3     0.647059        9                  1               0                  0   \n",
      "4     0.235294        4                  0               0                  0   \n",
      "\n",
      "   pe_D1_stud_med  pe_monthinc  pe_age  pe_female  predicted_attrition  \n",
      "0               0          400      21          1                    0  \n",
      "1               0          800      21          0                    0  \n",
      "2               0          800      23          0                    0  \n",
      "3               0          500      22          0                    0  \n",
      "4               1          350      19          1                    0  \n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
